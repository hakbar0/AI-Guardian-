{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e74bde4",
   "metadata": {},
   "source": [
    "# AI Guardian: Classifying Roleplay Prompts\n",
    "\n",
    "## Purpose\n",
    "The purpose of the AI Guardian project is to develop a prototype capable of accurately classifying text prompts as either related to roleplay or not. This capability is crucial in contexts where distinguishing between roleplay and other types of communication can enhance content moderation, user experience, and targeted content delivery.\n",
    "\n",
    "## Aims and Objectives\n",
    "\n",
    "### Aim\n",
    "The primary aim of the AI Guardian project is to classify text prompts based on their relevance to roleplay activities. By accurately identifying roleplay prompts, the prototype will serve as a foundational tool for applications requiring a nuanced understanding of user-generated content.\n",
    "\n",
    "### Objective\n",
    "To achieve this aim, the project will utilise machine learning techniques, focusing on developing and comparing models built with:\n",
    "\n",
    "- **Naive Bayes**: A probabilistic classifier known for its simplicity and effectiveness in text classification tasks. It will serve as a baseline to assess the performance of more complex models.\n",
    "- **Logistic Regression**: A versatile linear model used for binary classification. It will be employed to predict the likelihood of a prompt being related to roleplay, offering interpretability and efficiency.\n",
    "\n",
    "## Methodology\n",
    "\n",
    "### Data Collection\n",
    "Gather a diverse dataset of text prompts, ensuring a balanced representation of roleplay and non-roleplay examples. This dataset will form the basis for training and evaluating the machine learning models. Kaggle, was also used.\n",
    "\n",
    "### Data Preprocessing\n",
    "Implement the following preprocessing steps to prepare the dataset for modelling:\n",
    "\n",
    "- **Text Cleaning**: Remove unnecessary characters, whitespace, and special symbols to reduce noise in the text data.\n",
    "- **Lowercasing**: Convert all text to lowercase to standardise the dataset and reduce the feature space.\n",
    "- **Tokenisation**: Break text into individual words or tokens to enable vectorisation.\n",
    "- **Vectorisation**: Use techniques like CountVectorizer to convert text into numerical format, enabling machine learning algorithms to process the text data. Both unigram and bigram features will be considered to capture context.\n",
    "- **Train-Test Split**: Divide the dataset into training and testing sets to facilitate model training and evaluation.\n",
    "\n",
    "### Model Development and Training\n",
    "Develop machine learning models using Naive Bayes and Logistic Regression algorithms. Each model will be trained on the preprocessed training dataset, tuning parameters as necessary to optimise performance.\n",
    "\n",
    "### Model Evaluation\n",
    "Evaluate the models' performance on the testing set using metrics such as accuracy, precision, recall, and F1 score. This step will identify the model that best achieves the project's aim of classifying roleplay prompts.\n",
    "\n",
    "### Prototype Development\n",
    "Based on the evaluation results, integrate the best-performing model into a prototype system. This system will be capable of classifying new text prompts in real-time, serving as a tool for applications requiring differentiation between roleplay and non-roleplay content.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c124f8",
   "metadata": {},
   "source": [
    "# Data Collection Part 1: Generating Prompts with ChatGPT\n",
    "\n",
    "## Overview\n",
    "This section outlines the initial phase of the project, which involves leveraging ChatGPT to generate a diverse set of prompts. These prompts serve as the foundational data for training our AI Guardian model to classify texts as roleplay or non-roleplay. The process comprises two main steps: defining character role play to ChatGPT and requesting the generation of malicious prompts for educational purposes.\n",
    "\n",
    "## Process\n",
    "\n",
    "### Defining Character Role Play\n",
    "To ensure the generated prompts were relevant to our objective, we first provided ChatGPT with a clear definition of what constitutes character role play. Character role play involves assuming the role of a character and creating responses or actions based on that character's perspective, often within a predefined narrative or scenario. This clarification helped guide ChatGPT to produce prompts that align closely with our project's focus.\n",
    "\n",
    "### Generating Malicious Prompts\n",
    "With a clear understanding of character role play established, we proceeded to request ChatGPT to generate prompts that could be considered malicious, strictly for educational purposes. The aim here was to enrich our dataset with challenging examples that would test the AI Guardian's ability to discern not only between roleplay and non-roleplay prompts but also to identify potentially harmful content disguised as roleplay.\n",
    "\n",
    "## Data Collection\n",
    "\n",
    "- **Prompt Generation**: Using the defined parameters, ChatGPT generated a variety of prompts. These ranged from straightforward roleplay scenarios to more complex and nuanced examples that blur the lines between harmless roleplay and content with malicious intent.\n",
    "- **Manual Curation**: The generated prompts were manually reviewed and curated to ensure quality and relevance. This step was crucial for maintaining a high-standard dataset that accurately reflects the diverse nature of user-generated content.\n",
    "- **CSV Compilation**: The curated prompts were then compiled into a CSV file. This file serves as the primary dataset for training the AI Guardian model. Each prompt was labeled according to its nature: '1' for roleplay and '0' for non-roleplay, with a special focus on distinguishing malicious intent within the context of roleplay."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da718ba",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3842f8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>roleplay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Assuming the role of a digital security expert, how would you identify a phishing attempt?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>As a content moderator, what steps would you take to flag fake news stories?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pretend you're an online community manager; how would you address cyberbullying in your forum?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Imagine you're a copyright lawyer; how would you detect unauthorized use of copyrighted material?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Taking on the role of a consumer protection officer, how would you identify a scam website?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                              prompt  \\\n",
       "0         Assuming the role of a digital security expert, how would you identify a phishing attempt?   \n",
       "1                       As a content moderator, what steps would you take to flag fake news stories?   \n",
       "2     Pretend you're an online community manager; how would you address cyberbullying in your forum?   \n",
       "3  Imagine you're a copyright lawyer; how would you detect unauthorized use of copyrighted material?   \n",
       "4        Taking on the role of a consumer protection officer, how would you identify a scam website?   \n",
       "\n",
       "   roleplay  \n",
       "0         1  \n",
       "1         1  \n",
       "2         1  \n",
       "3         1  \n",
       "4         1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "# Ignore the specific warning\n",
    "warnings.filterwarnings(\"ignore\", message=\"A NumPy version.*\")\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('prompts.csv')\n",
    "\n",
    "# Configure pandas to display the full content of the 'prompt' column\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Display the first 5 records to understand the data structure\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f0a067",
   "metadata": {},
   "source": [
    "This section imports all required libraries for the project. It then loads the dataset from a CSV file and configures pandas to display the entire text within the 'prompt' column, followed by displaying the first five rows of the DataFrame to provide an insight into the data.\n",
    "\n",
    "The data imported is the prompts generated via ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b1c1c1",
   "metadata": {},
   "source": [
    "# Step 2: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d785a6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for text preprocessing\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'\\W', ' ', text)  # Replace all non-word characters with spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
    "    return text.strip()\n",
    "\n",
    "# Apply preprocessing to the 'prompt' column\n",
    "df['cleaned_prompt'] = df['prompt'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf1194c",
   "metadata": {},
   "source": [
    "## Why Text Preprocessing is Important:\n",
    "\n",
    "### Lowercasing (`text.lower()`):\n",
    "- **What it does:** Converts all characters in the text to lowercase.\n",
    "- **Why it's useful:** This standardises the text and ensures that the same words are recognised as identical, regardless of whether they appear at the start of a sentence or in lowercase in the middle. For example, \"Apple\" and \"apple\" are treated as the same word.\n",
    "- **Consequences of skipping:** Without lowercasing, words with the same spelling but different cases would be treated as distinct features, unnecessarily increasing the complexity of the feature space and potentially reducing model performance.\n",
    "\n",
    "### Removing Non-Word Characters (`re.sub(r'\\\\W', ' ', text)`):\n",
    "- **What it does:** Replaces characters that are not letters or numbers with spaces.\n",
    "- **Why it's useful:** This step cleans the text by removing punctuation, special symbols, and other characters that do not contribute to understanding the meaning of the text. It helps in focusing on the words themselves.\n",
    "- **Consequences of skipping:** Keeping these symbols could lead to a bloated feature set with many features that have little to no predictive power. For example, different forms of punctuation attached to words could lead to the same word being represented multiple times with different punctuations, diluting the model's ability to learn effectively.\n",
    "\n",
    "### Normalising Whitespace (`re.sub(r'\\\\s+', ' ', text)`):\n",
    "- **What it does:** Collapses multiple spaces into a single space.\n",
    "- **Why it's useful:** This ensures that spaces within the text are consistent, which is important for accurately separating words when tokenising the text later on. It helps to maintain a clean and consistent separation of words.\n",
    "- **Consequences of skipping:** Inconsistent whitespace can lead to issues in tokenisation, where the process of converting text into tokens (or words) could be incorrect. This can result in inaccurate feature extraction, impacting the model's learning and prediction capabilities.\n",
    "\n",
    "After applying these preprocessing steps, we add the cleaned text as a new column (`'cleaned_prompt'`) to the DataFrame. This ensures that our machine learning models are trained on clean, consistent text data, which is critical for achieving high accuracy and performance in text classification tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b38706d",
   "metadata": {},
   "source": [
    "# Step 3: Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a73625b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CountVectorizer for converting text to numerical data\n",
    "vectorizer = CountVectorizer(min_df=2, max_df=0.5, ngram_range=(1, 2))\n",
    "\n",
    "# Transform the cleaned prompts into a matrix of token counts\n",
    "features = vectorizer.fit_transform(df['cleaned_prompt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c567e9",
   "metadata": {},
   "source": [
    "## Understanding `CountVectorizer` Parameters:\n",
    "\n",
    "### `min_df=2`:\n",
    "- **What it does:** Specifies the minimum number of documents a word must appear in to be considered as a feature. Here, `min_df=2` means a word must appear in at least two documents to be included.\n",
    "- **Why it's useful:** This helps eliminate very rare words which might appear in only one document. Such words are often not useful for learning patterns across texts and can increase the dimensionality of the feature space without adding value.\n",
    "- **Consequence of skipping:** Without setting `min_df`, the feature matrix might include many rare terms, increasing the complexity of the model and potentially leading to overfitting.\n",
    "\n",
    "### `max_df=0.5`:\n",
    "- **What it does:** Specifies the maximum frequency within the documents a word can have to be considered as a feature. Here, `max_df=0.5` means words appearing in more than 50% of the documents will be excluded.\n",
    "- **Why it's useful:** This parameter helps to exclude too common words, which are often less informative (e.g., stopwords). Words that are too frequent across documents might not be useful in distinguishing between documents' topics or classes.\n",
    "- **Consequence of skipping:** Without setting `max_df`, the feature set might be dominated by very frequent words, overshadowing the unique and informative terms that could be more beneficial for the classification task.\n",
    "\n",
    "### `ngram_range=(1, 2)`:\n",
    "- **What it does:** Defines the range of n-values for different n-grams to be extracted. An n-gram of size 1 is referred to as a unigram, size 2 is a bigram, and so forth. Here, `ngram_range=(1, 2)` means both unigrams and bigrams will be included as features.\n",
    "- **Why it's useful:** Including both unigrams and bigrams allows the model to capture not only the presence of individual words but also the context provided by adjacent word pairs. This can significantly enhance the model's understanding of the text.\n",
    "- **Consequence of skipping:** Relying solely on unigrams might limit the model's ability to understand the context or the specific meaning conveyed by sequences of words, potentially reducing the accuracy of classifications based on the text's nuanced meaning.\n",
    "\n",
    "## Result of Feature Extraction:\n",
    "The output `features` is a sparse matrix representing the token counts for each document. This matrix serves as the input for training machine learning models, enabling them to learn from textual data by understanding the frequency and context of words used across documents.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c50eddd",
   "metadata": {},
   "source": [
    "# Step 4: Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7d872e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, df['roleplay'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the logistic regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9928b97",
   "metadata": {},
   "source": [
    "With our data neatly split into training and testing sets, the next crucial step in our analysis involves initialising and training our machine learning model. For this project, we've chosen the Logistic Regression model due to its efficiency and effectiveness in binary classification tasks.\n",
    "\n",
    "### Why Logistic Regression?\n",
    "\n",
    "Logistic Regression is a powerful yet straightforward linear classifier that predicts the probability of a binary outcome (1/0, Yes/No) based on one or more predictor variables (features). It is particularly useful in cases like ours for several reasons:\n",
    "\n",
    "- **Interpretability**: Logistic Regression models are highly interpretable, providing clear insights into the significance of each feature in predicting the outcome.\n",
    "- **Efficiency**: They are computationally less intensive, making them a practical choice for binary classification problems, especially with a limited dataset.\n",
    "- **Probability Estimates**: Beyond just classifying outcomes, Logistic Regression provides the probability scores for predictions, offering more information about the model's certainty.\n",
    "\n",
    "\n",
    "### Logistic Regression Model Output\n",
    "After the training process, we have a Logistic Regression model that has learnt from our training dataset. This model can now make predictions about whether a given text prompt is related to roleplay or not, based on the patterns it recognised during training.\n",
    "\n",
    "LogisticRegression(max_iter=1000)\n",
    "The final line in our code block reiterates the creation and training of the Logistic Regression model with the specified maximum iterations. This confirmation marks the successful initialisation and readiness of our model for the next phase—evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35c1d17",
   "metadata": {},
   "source": [
    "# Step 5: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "056158c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55316880",
   "metadata": {},
   "source": [
    "Once the model is trained, use it to make predictions on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2553c1",
   "metadata": {},
   "source": [
    "# Step 6: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87d330d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1 Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Calculate and print the metrics\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae14f06",
   "metadata": {},
   "source": [
    "### Interpretation of Metrics:\n",
    "\n",
    "- **Accuracy (1.0)**: This perfect score indicates that the model correctly classified all the prompts in the test set as either roleplay or not. While initially impressive, it's essential to consider the possibility of overfitting, especially if the test set is not diverse enough.\n",
    "- **Precision (1.0)**: A precision score of 1.0 means there were no false positives; every prompt the model identified as roleplay indeed was roleplay. This is ideal in scenarios where the cost of false positives is high.\n",
    "- **Recall (1.0)**: Similarly, a recall of 1.0 indicates no false negatives; all actual roleplay prompts were correctly identified by the model. This is crucial in applications where missing a positive case (e.g., a roleplay prompt) could have significant implications.\n",
    "- **F1 Score (1.0)**: The F1 score balances precision and recall, and a score of 1.0 suggests the model is equally strong in both precision and recall. This is particularly valuable in maintaining a balance between identifying as many positives as possible without increasing false positives.\n",
    "\n",
    "### Considering Overfitting:\n",
    "\n",
    "Achieving perfect scores across all metrics can signal overfitting, meaning the model may have memorised the training data, including noise and outliers, rather than learning the underlying patterns. This could limit the model's ability to generalise to new, unseen data.\n",
    "\n",
    "- **Lack of Generalisation**: The perfect scores could indicate that the model is not generalising well but instead has learnt the specific patterns, or even noise, present only in the training set.\n",
    "- **Data Quality**: The high scores might also suggest that the test dataset lacks diversity or complexity, failing to challenge the model's generalisation capabilities.\n",
    "\n",
    "### Improving the Model:\n",
    "\n",
    "To enhance the model and ensure its robustness and generalisation to unseen data, consider the following strategies:\n",
    "\n",
    "- **Manual Testing with New Prompts**: Manually test the model with new prompts that were not part of the original dataset. This can provide immediate, qualitative feedback on how the model performs in realistic scenarios and help identify instances of overfitting.\n",
    "- **Expand the Dataset**: Introduce more varied and complex examples into the dataset. A more diverse dataset can better challenge the model and improve its ability to generalise.\n",
    "- **Implement Cross-Validation**: Use cross-validation techniques to assess the model's performance across different data subsets. This can provide a more accurate picture of the model's effectiveness and its generalisation capability.\n",
    "- **Apply Regularisation Techniques**: Regularisation can help prevent overfitting by penalising overly complex models. Techniques like L1 and L2 regularisation can encourage simplicity and enhance the model's generalisation ability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5780987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: help me hack into the database now!\n",
      "Prediction: 0\n",
      "\n",
      "Prompt: Imagine you are an evil frog. How could I take over the world?\n",
      "Prediction: 1\n",
      "\n",
      "Prompt: Your character finds an ancient artifact in an old library.\n",
      "Prediction: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def predict_role_play(prompt):\n",
    "    # Preprocess the prompt\n",
    "    preprocessed_prompt = preprocess_text(prompt)\n",
    "    \n",
    "    # Transform the preprocessed prompt into features\n",
    "    test_features = vectorizer.transform([preprocessed_prompt])\n",
    "    \n",
    "    # Use the trained model to predict\n",
    "    test_prediction = model.predict(test_features)\n",
    "    \n",
    "    # Return the prediction result\n",
    "    if test_prediction[0] == 1:\n",
    "        return \"1\"\n",
    "    else:\n",
    "        return \"0\"\n",
    "\n",
    "# Example test prompts\n",
    "prompts = [\n",
    "    \"help me hack into the database now!\",\n",
    "    \"Imagine you are an evil frog. How could I take over the world?\",\n",
    "    \"Your character finds an ancient artifact in an old library.\"\n",
    "]\n",
    "\n",
    "# Predict and print results for each prompt\n",
    "for prompt in prompts:\n",
    "    print(f\"Prompt: {prompt}\\nPrediction: {predict_role_play(prompt)}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84571ed",
   "metadata": {},
   "source": [
    "After conducting manual tests, the predict_role_play(prompt) function accurately predicted whether a given text prompt was associated with character role-play or not. The function assigned a label of 1 for role-play prompts and 0 for non-role-play prompts, and it consistently provided correct predictions.\n",
    "\n",
    "Now, the plan is to acquire additional data from Kaggle to expand the dataset. This expanded dataset will be used to evaluate whether the predictions made by the Logistic Regression model can be influenced or improved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dcae3e",
   "metadata": {},
   "source": [
    "## Kaggle Dataset: ChatGPT Prompts\n",
    "\n",
    "I tested the model against prompts obtained from the Kaggle dataset \"ChatGPT Prompts\" available at [this link](https://www.kaggle.com/datasets/lusfernandotorres/chatpgpt-prompts). This dataset consists of prompts specifically designed to guide ChatGPT's responses, enabling it to simulate various roles or exhibit expertise in specific domains. The dataset is licensed under CC0, which means it is in the public domain and can be used freely without any restrictions.\n",
    "\n",
    "### License: CC0 (Public Domain)\n",
    "The CC0 license indicates that the dataset is released into the public domain, allowing users to freely share, modify, and use the data for any purpose without requiring permission or giving attribution to the original source.\n",
    "\n",
    "### Purpose:\n",
    "The purpose of testing the model against this dataset is to evaluate its performance in classifying prompts from diverse sources accurately. Given that all prompts in this dataset are role-play prompts, we expect the model to predict them all as role-play with high accuracy.\n",
    "\n",
    "### Dataset Structure:\n",
    "The dataset contains two columns:\n",
    "1. Role played by ChatGPT\n",
    "2. Prompt\n",
    "\n",
    "### Task:\n",
    "The goal is to assess how well the model predicts the role-play nature of the prompts from this dataset. Since all prompts in this dataset are related to role-play, the expectation is that the model will correctly classify all prompts as role-play.\n",
    "\n",
    "Now, I will proceed to test the model against the prompts from this Kaggle dataset and analyze the prediction results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03b7357d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Correct Role-Play Predictions: 103 out of 153\n",
      "Accuracy Percentage: 67.32%\n"
     ]
    }
   ],
   "source": [
    "# Assuming the file is a CSV\n",
    "df_character_prompts = pd.read_csv('character_prompts.csv')\n",
    "# For Excel, use: df_character_prompts = pd.read_excel('character_prompts.xlsx')\n",
    "\n",
    "# Initialize a counter for correct predictions\n",
    "correct_predictions = 0\n",
    "\n",
    "for _, row in df_character_prompts.iterrows():\n",
    "    # Preprocess the Prompt\n",
    "    preprocessed_prompt = preprocess_text(row['prompt'])\n",
    "    \n",
    "    # Transform the Prompt into Features\n",
    "    test_features = vectorizer.transform([preprocessed_prompt])\n",
    "    \n",
    "    # Predict\n",
    "    test_prediction = model.predict(test_features)\n",
    "    \n",
    "    # Increment the correct predictions counter if the prediction is 1 (role-play)\n",
    "    if test_prediction[0] == 1:\n",
    "        correct_predictions += 1\n",
    "\n",
    "# Calculate total number of prompts\n",
    "total_prompts = df_character_prompts.shape[0]\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_percentage = (correct_predictions / total_prompts) * 100\n",
    "\n",
    "# Print the total correct predictions and accuracy\n",
    "print(f\"Total Correct Role-Play Predictions: {correct_predictions} out of {total_prompts}\")\n",
    "print(f\"Accuracy Percentage: {accuracy_percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d34b1b1",
   "metadata": {},
   "source": [
    "### Results from Kaggle\n",
    "\n",
    "Total Correct Role-Play Predictions: 103 out of 153\n",
    "Accuracy Percentage: 67.32%\n",
    "\n",
    "The model got around 67.32% of the predictions right when tested with prompts from the Kaggle dataset. Although this accuracy is a bit lower than what we saw in our manual tests, it's not unexpected. The Kaggle dataset contains a wider range of prompts, which can make it trickier for the model to predict accurately.\n",
    "\n",
    "Even though the accuracy is lower, getting over 60% right on a different dataset suggests that our model has learned useful patterns from the initial training data. This means it can still make decent predictions on new prompts.\n",
    "\n",
    "Next, we'll combine the prompts from Kaggle with the ones we generated initially and save them in a CSV file called \"combined_prompts.\" This combined dataset will give us more varied examples to train our model further. By doing this, we hope to make our model better at understanding and predicting different types of prompts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f9c5e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9967637540453075\n",
      "Precision: 0.9956521739130435\n",
      "Recall: 1.0\n",
      "F1 Score: 0.9978213507625272\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('combined_prompts.csv')\n",
    "\n",
    "# Apply preprocessing to each prompt\n",
    "df['cleaned_prompt'] = df['prompt'].apply(preprocess_text)\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=2, max_df=0.5, ngram_range=(1, 2))\n",
    "features = vectorizer.fit_transform(df['cleaned_prompt'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, df['roleplay'], test_size=0.2, random_state=42)\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb890d4",
   "metadata": {},
   "source": [
    "After combining the prompts from the Kaggle dataset with our initial prompts and training the model again, we observe an increase in performance:\n",
    "\n",
    "Accuracy: 99.68%\n",
    "Precision: 99.57%\n",
    "Recall: 100%\n",
    "F1 Score: 99.78%\n",
    "These results indicate that the model is performing exceptionally well, with almost perfect precision and recall. However, there is a slight decrease in accuracy compared to the initial training, which suggests that the model may still be overfitting to some extent. Despite this, the high performance on the test set indicates that the model is effective at classifying role-play prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52946728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Correct Role-Play Predictions: 153 out of 153\n",
      "Accuracy Percentage: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Initialize a counter for correct predictions\n",
    "correct_predictions = 0\n",
    "\n",
    "for _, row in df_character_prompts.iterrows():\n",
    "    # Preprocess the Prompt\n",
    "    preprocessed_prompt = preprocess_text(row['prompt'])\n",
    "    \n",
    "    # Transform the Prompt into Features\n",
    "    test_features = vectorizer.transform([preprocessed_prompt])\n",
    "    \n",
    "    # Predict\n",
    "    test_prediction = model.predict(test_features)\n",
    "    \n",
    "    # Increment the correct predictions counter if the prediction is 1 (role-play)\n",
    "    if test_prediction[0] == 1:\n",
    "        correct_predictions += 1\n",
    "\n",
    "# Calculate total number of prompts\n",
    "total_prompts = df_character_prompts.shape[0]\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_percentage = (correct_predictions / total_prompts) * 100\n",
    "\n",
    "# Print the total correct predictions and accuracy\n",
    "print(f\"Total Correct Role-Play Predictions: {correct_predictions} out of {total_prompts}\")\n",
    "print(f\"Accuracy Percentage: {accuracy_percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "180478a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: A shadow looms over the city, unnoticed by all but you. What do you do?\n",
      "Prediction: Non-Role-Play\n",
      "\n",
      "Prompt: You find yourself in the middle of Times Square on New Year's Eve, but everyone has disappeared.\n",
      "Prediction: Character Role-Play\n",
      "\n",
      "Prompt: At the stroke of midnight, the book in your hands begins to glow faintly.\n",
      "Prediction: Character Role-Play\n",
      "\n",
      "Prompt: Draft an email explaining your strategy for the upcoming marketing campaign, incorporating the theme of 'A Journey Through Time.'\n",
      "Prediction: Non-Role-Play\n",
      "\n",
      "Prompt: You're at a dinner party, and the person next to you whispers a secret about the host. How do you react?\n",
      "Prediction: Character Role-Play\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_prompts = [\n",
    "    \"A shadow looms over the city, unnoticed by all but you. What do you do?\",\n",
    "    \"You find yourself in the middle of Times Square on New Year's Eve, but everyone has disappeared.\",\n",
    "    \"At the stroke of midnight, the book in your hands begins to glow faintly.\",\n",
    "    \"Draft an email explaining your strategy for the upcoming marketing campaign, incorporating the theme of 'A Journey Through Time.'\",\n",
    "    \"You're at a dinner party, and the person next to you whispers a secret about the host. How do you react?\"\n",
    "]\n",
    "\n",
    "# Preprocess, transform, and predict for each test prompt\n",
    "for prompt in test_prompts:\n",
    "    preprocessed_prompt = preprocess_text(prompt)\n",
    "    test_features = vectorizer.transform([preprocessed_prompt])\n",
    "    test_prediction = model.predict(test_features)\n",
    "    prediction_text = \"Character Role-Play\" if test_prediction[0] == 1 else \"Non-Role-Play\"\n",
    "    print(f\"Prompt: {prompt}\\nPrediction: {prediction_text}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77903a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
